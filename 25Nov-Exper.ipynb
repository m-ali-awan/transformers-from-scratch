{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47239ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06fd572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /opt/conda/lib/python3.8/site-packages (0.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5972e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a89214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1867b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    To split images into patches of given size, and convert to embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    img_size: int   (Size of the image)\n",
    "    \n",
    "    patch_size: int  (Size of Patch)\n",
    "    \n",
    "    in_chans: int (No of input channels)\n",
    "    \n",
    "    embedding_dims: int (The embeddings dimension)\n",
    "    \n",
    "    \n",
    "    Attributes:\n",
    "    -------\n",
    "    n_patches : int   (No of patches inside the image)\n",
    "    \n",
    "    proj: nn.Conv2d\n",
    "        Convolution Layer that do both the splitting into patches, and their embeddings\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,img_size,patch_size,in_chans=3,embedding_dims=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size=img_size\n",
    "        self.patch_size=patch_size\n",
    "        self.n_patches=(img_size//patch_size)**2\n",
    "        \n",
    "        \n",
    "        self.proj=nn.Conv2d(in_chans,embedding_dims,\n",
    "                           kernel_size=patch_size,\n",
    "                           stride=patch_size\n",
    "                           )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        \"\"\"\n",
    "        parameters:\n",
    "        ---------\n",
    "        x: torch.Tensor   Shape '(n_samples,in_chans, img_size,img_size)'\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        torch.Tensor  Shape '(n_samples,n_patches, embed_dims)'\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        x=self.proj(\n",
    "                    x\n",
    "                    )  # (n_samples,embed_dims,n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x=x.flatten(2)\n",
    "        x=x.transpose(1,2) # (n_samples,n_patches, embed_dims)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02556b7b",
   "metadata": {},
   "source": [
    "# Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "606ffd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([3, 7, 4]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to del\n",
    "x=torch.ones(3,5,5)\n",
    "x2=torch.ones(3,7,4)\n",
    "x.shape,x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b128508",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv=nn.Conv2d(3,768,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe865cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=test_conv(x.unsqueeze(0))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a29c6095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 3, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2=test_conv(x2.unsqueeze(0))\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c9f8fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2=out2.flatten(2)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7db24",
   "metadata": {},
   "source": [
    "* **So we can also design transformers to work with rectangle images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc485d69",
   "metadata": {},
   "source": [
    "# Implementing Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db9ac0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Mechanism\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    dim: int  Input and output ims of per patch(or token) features\n",
    "    \n",
    "    n_heads: int   No of attention heads\n",
    "    \n",
    "    qkv_bias: True  If True, then we include bias to the query, key, and value projections\n",
    "    \n",
    "    attn_dp: float  Dropout prob applied to the k,q,and v tensors\n",
    "    \n",
    "    fin_proj_dp: float Dropout prob applied to the output tensor\n",
    "    \n",
    "    Attributes:\n",
    "    --------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dim,n_heads=12,qkv_bias=True, attn_dp=0.0, fin_proj_dp=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim=dim\n",
    "        self.n_heads=n_heads\n",
    "        self.head_dim= dim//n_heads\n",
    "        self.scale=self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv=nn.Linear(dim,dim*3,bias=qkv_bias)\n",
    "        self.attn_dp=nn.Dropout(attn_dp)\n",
    "        self.fin_proj=nn.Linear(dim,dim)\n",
    "        self.fin_proj_dp=nn.Dropout(fin_proj_dp)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run Forward Pass\n",
    "        \"\"\"\n",
    "        \n",
    "        n_samples,n_tokens,dim=x.shape\n",
    "        qkv=self.qkv(x) # (n_samples,n_patches+1,dim*3)\n",
    "        qkv=qkv.reshape(\n",
    "                        n_samples,n_tokens,3,self.n_heads,self.head_dim\n",
    "        )\n",
    "        qkv=qkv.permute(2,0,3,1,4)  # (3,n_samples,n_heads,n_patches+1,head_dim)\n",
    "        k_t=k.transpose(-2,-1) # (n_samples,n_heads,head_dim,n_patches+1)\n",
    "        dp=(q @ k_t) * self.scale   # (n_samples,n_heads, n_patches+1, n_patches+1)\n",
    "        attn=dp.softmax(dim=-1)\n",
    "        attn=self.attn_dp(attn)\n",
    "        \n",
    "        weighted_avg=attn @ v   # (n_samples,n_heads, n_patches+1, heads_dim)\n",
    "        weighted_avg=weighted_avg.transpose(1,2)  # (n_samples,n_patches+1,n_heads,heads_dim)\n",
    "        \n",
    "        weighted_avg=weighted_avg.flatten(2)  # (n_samples,n_patches+1,dim)\n",
    "        x=self.fin_proj(weighted_avg)\n",
    "        x=self.fin_proj_dp(x)\n",
    "        \n",
    "        # So we returned the same shape as input was of.\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87b7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
